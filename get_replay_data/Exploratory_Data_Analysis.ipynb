{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis on a single SC2 Replay File\n",
    "\n",
    "This notebook details the process for loading and exploring a single SC2 replay file. The following goals are achieved:\n",
    "1. Load the replay file using `sc2reader`\n",
    "2. Extract the events stored in the `.events` attribute of the replay file into a pandas dataframe.\n",
    "3. Extract other pertinent information from the replay file such as: \n",
    "    * Filehash.\n",
    "    * Datetime the game was played.\n",
    "    * The map on which the game was played.\n",
    "    * Length of the game.\n",
    "    * Races of the players.\n",
    "    * Skill levels of the players (by league).\n",
    "    * The winner of the game.\n",
    "4. Drop all unnecessary columns, convert the rest of the columns into dummy variables which indicate the category of the action and 1 if the action was taken.\n",
    "5. Divide the data up into time chunks (10 seconds initially, to be revised later). Sum the number of 1s in each chunk to get the number of actions taken in each chunk.\n",
    "6. Add columns to the dataframe indicating the number of actions taken in all previous chunks.\n",
    "7. Add columns to the dataframe indicating player races, skill levels, and game winner.\n",
    "8. Attempt a logistic regression using all chunks as inputs.\n",
    "9. Investigate an optimization method which results in the earliest prediction that does not change until the end of the game. I.e., how early can the regression make a prediction which remains constant until the end of the game.\n",
    "\n",
    "See __[this link](https://web.archive.org/web/20201031184319/https://miguelgondu.github.io/python/ai/video%20games/2018/09/04/a-tutorial-on-sc2reader-events-and-units.html )__ for the tutorial that was used in establishing how to work with, and extract data from an SC2 replay file, using `sc2reader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and settings\n",
    "\n",
    "import pandas as pd\n",
    "import sc2reader\n",
    "import json\n",
    "import datetime\n",
    "from multiprocessing import Pool # for parallel processing of replay files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Replay\n",
    "We will use the `load_level=4` setting to extract the most detailed data possible from the replay file. This includes all events, units, and upgrades.\n",
    "\n",
    "The replay selected is a professional replay between Serral (Zerg) and Showtime (Protoss). The replay is 42 minutes long (which is long for a StarCraft II game) which should be a good indication of the upper end of the processing time and requirements for a replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a replay\n",
    "replay = sc2reader.load_replay(\n",
    "    '../data/replays/SpawningTool/Pro/page1/2021-09-05-zserral-vs-pshowtime.SC2Replay',\n",
    "    load_level=4)\n",
    "\n",
    "# Extract a list of replay events\n",
    "events_list = replay.events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Event Data from Replay\n",
    "The following code extracts the events from the replay file and stores them in a pandas dataframe. Multiproccessing is used to speed up the process, as the replay file contains over 150,000 events.\n",
    "\n",
    "Steps:\n",
    "1. `events_list` now contains a list of stored event objects. `get_event_data` is a function that takes a specific event, and returns a dictionary of the values for each attribute of that event object. \n",
    "2. A multiprocessing pool iterates through all events returning a dictionary for each.\n",
    "3. The output from the pool is stored in a list of dictionaries which can then be compressed into a pandas dataframe. A list of dictionaries is used so that all column names for all events do not need to be created manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to get the event data so that multiprocessing can be used\n",
    "def get_event_data(event):\n",
    "    \"\"\"\n",
    "    get_event_data\n",
    "    Extracts the data from the event and returns it as a dictionary\n",
    "    Ignores events that start with '_', i.e., special attributes and dunder types\n",
    "\n",
    "    Args:\n",
    "        event (sc2reader.event): Event object extracted from sc2reader.events\n",
    "\n",
    "    Returns:\n",
    "        [dict]: A dictionary containing the event data\n",
    "    \"\"\"    \n",
    "    # ignore attributes that are not needed (special or dunder)\n",
    "    event_attributes = [attr for attr in dir(event) if not attr.startswith('_')]\n",
    "\n",
    "    # initialize a dictionary to store the values of each attribute\n",
    "    d = dict()\n",
    "\n",
    "    # loop through each attribute and store the value in the dictionary\n",
    "    for attr in event_attributes:\n",
    "        # ignore attributes if they do not contain a value type\n",
    "        if type(getattr(event, attr)) in [int, float, str, bool]:\n",
    "            d[attr] = getattr(event, attr)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151759, 112)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the function on each event in the list, returns a list of dictionaries\n",
    "with Pool() as pool:\n",
    "    dict_list = pool.map(get_event_data, events_list)\n",
    "\n",
    "# convert the list of dictionaries to a dataframe\n",
    "event_df = pd.DataFrame(dict_list)\n",
    "event_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We not have an `event_df` with 151,759 rows and 112 columns.\n",
    "\n",
    "## Processing the Dataframe\n",
    "The major steps to accomplish here are:\n",
    "1. Identify all columns.\n",
    "2. Drop all columns that are not needed. We will also store a list of columns that are left after dropping in a csv file in the info directory. This list can be used later as a master list of columns for processing of all dataframes.\n",
    "3. Check if columns are continuous or categorical (discrete integers are considered categorical).\n",
    "4. Convert all categorical columns into dummy variables and drop the original column, and the first dummy column (using the pandas `drop first` option).\n",
    "5. Remove all rows with non-player PID values (the players are the first two PIDs in the replay, all other PIDs are observers).\n",
    "6. Summarise the dataframe by dividing it up into chunks of 10 seconds, summing up the count of all events during that time.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4cf022966bbd36933c612b9a537128a7afa164db307dc826422466b94a3d27f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('sc2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
